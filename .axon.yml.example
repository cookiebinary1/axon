llm:
  base_url: "http://127.0.0.1:8080"
  model: "qwen2.5-coder-3b"
  temperature: 0.15

server:
  # Automatically start llama-server when axon starts
  # If server is not running, you'll be prompted to select a model
  auto_start: true
  # Path to llama-server binary (leave empty to use from PATH)
  server_path: ""
  # Default model (you can override via interactive selection)
  model: "Qwen/Qwen2.5-Coder-3B-Instruct-GGUF:Q4_K_M"

context:
  # Files and directories to ignore when processing project context
  # You can use simple prefix matches or glob patterns
  ignore:
    - "vendor/"
    - "node_modules/"
    - "storage/"
    - ".git/"
    - "dist/"
    - "build/"

